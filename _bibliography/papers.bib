@article{feng2025canmllms,
  abbr={Arxiv},
  title={Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps},
  author={Feng†, Sicheng and Wang†, Song and Ouyang, Shuyi and Kong, Lingdong and Song, Zikai and Zhu, Jianke and Wang*, Huan and Wang, Xinchao},
  journal={Arxiv},
  year={2025},
  month={May},
  webpage={https://fscdc.github.io/Reason-Map/},
  arxiv={https://arxiv.org/abs/2505.18675},
  code={https://github.com/fscdc/ReasonMap},
  dataset={https://huggingface.co/datasets/FSCCS/ReasonMap},
  qbitai={https://mp.weixin.qq.com/s/sPJLQtHgl5DZghWLWa_H3Q},
  annotation={† Equal contribution. * Corresponding authors.},
  preview={reasonmap.png},
  selected={true},
  abstract={Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.},
}

@article{feng2025efficient,
  abbr={Arxiv},
  title={Efficient Reasoning Models: A Survey},
  author={Feng, Sicheng and Fang, Gongfan and Ma, Xinyin and Wang*, Xinchao},
  journal={Arxiv},
  year={2025},
  month={April},
  code={https://github.com/fscdc/Awesome-Efficient-Reasoning-Models},
  arxiv={https://arxiv.org/abs/2504.10903},
  annotation={* Corresponding authors},
  preview={efficient-survey.png},
  selected={true},
  abstract={Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this "slow-thinking" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.}
}

@article{feng2024oracle,
  abbr={Arxiv},
  title={Is Oracle Pruning the True Oracle?},
  author={Feng, Sicheng and Tao, Keda and Wang*, Huan},
  journal={Arxiv},
  year={2024},
  month={Dec},
  webpage={https://fscdc.github.io/Oracle-Pruning-Sanity-Check/},
  arxiv={https://arxiv.org/abs/2412.00143},
  code={https://github.com/fscdc/Oracle-Pruning-Sanity-Check},
  annotation={* Corresponding authors},
  preview={oracle.png},
  selected={true},
  abstract={Oracle pruning, which selects unimportant weights by minimizing the pruned train loss, has been taken as the foundation for most neural network pruning methods for over 35 years, while few (if not none) have thought about how much the foundation really holds. This paper, for the first time, attempts to examine its validity on modern deep models through empirical correlation analyses and provide reflections on the field of neural network pruning. Specifically, for a typical pruning algorithm with three stages (pertaining, pruning, and retraining), we analyze the model performance correlation before and after retraining. Extensive experiments (37K models are trained) across a wide spectrum of models (LeNet5, VGG, ResNets, ViT, MLLM) and datasets (MNIST and its variants, CIFAR10/CIFAR100, ImageNet-1K, MLLM data) are conducted. The results lead to a surprising conclusion: on modern deep learning models, the performance before retraining is barely correlated with the performance after retraining. Namely, the weights selected by oracle pruning can hardly guarantee a good performance after retraining. This further implies that existing works using oracle pruning to derive pruning criteria may be groundless from the beginning. Further studies suggest the rising task complexity is one factor that makes oracle pruning invalid nowadays. Finally, given the evidence, we argue that the retraining stage in a pruning algorithm should be accounted for when developing any pruning criterion.}
}

@article{feng2024unveiling,
  abbr={Cell Discov},
  title={Unveiling potential threats: backdoor attacks in single-cell pre-trained models.},
  author={Feng, Sicheng and Li, Siyu and Chen*, Luonan and Chen*, Shengquan},
  journal={Cell Discovery},
  volume={10},
  pages={122},
  year={2024},
  month={Nov},
  doi={10.1038/s41421-024-00753-1},
  webpage={https://fscdc.github.io/scBackdoor/},
  pdf={https://www.nature.com/articles/s41421-024-00753-1},
  code={https://github.com/BioX-NKU/scBackdoor},
  seqchina={https://mp.weixin.qq.com/s/n-TP8O1zQAy8ITBJewWARg},
  additional_info={Q1 TOP, IF=14.8},
  annotation={* Corresponding authors},
  preview={backdoor.png},
  selected={true},
}