@article{feng2025canmllms,
  abbr={Arxiv},
  title={Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps},
  author={Feng†, Sicheng and Wang†, Song and Ouyang, Shuyi. and Kong, Lingdong. and Song, Zikai. and Zhu, Jianke. and Wang*, Huan. and Wang, Xinchao.},
  journal={Arxiv},
  year={2025},
  month={May},
  webpage={https://fscdc.github.io/Reason-Map/},
  arxiv={https://arxiv.org/abs/2505.18675},
  code={https://github.com/fscdc/ReasonMap},
  dataset={https://huggingface.co/datasets/FSCCS/ReasonMap},
  qbitai={https://mp.weixin.qq.com/s/sPJLQtHgl5DZghWLWa_H3Q},
  annotation={† Equal contribution. * Corresponding authors.},
  preview={reasonmap.png},
  selected={true},
  abstract={Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.},
}

@article{feng2025efficient,
  abbr={Arxiv},
  title={Efficient Reasoning Models: A Survey},
  author={Feng, Sicheng. and Fang, Gongfan. and Ma, Xinyin. and Wang*, Xinchao.},
  journal={Arxiv},
  year={2025},
  month={April},
  code={https://github.com/fscdc/Awesome-Efficient-Reasoning-Models},
  arxiv={https://arxiv.org/abs/2504.10903},
  annotation={* Corresponding authors},
  selected={true},
}

@article{feng2024oracle,
  abbr={Arxiv},
  title={Is Oracle Pruning the True Oracle?},
  author={Feng, Sicheng. and Tao, Keda. and Wang*, Huan.},
  journal={Arxiv},
  year={2024},
  month={Dec},
  webpage={https://fscdc.github.io/Oracle-Pruning-Sanity-Check/},
  arxiv={https://arxiv.org/abs/2412.00143},
  code={https://github.com/fscdc/Oracle-Pruning-Sanity-Check},
  annotation={* Corresponding authors},
  selected={true},
}

@article{feng2024unveiling,
  abbr={Cell Discov},
  title={Unveiling potential threats: backdoor attacks in single-cell pre-trained models.},
  author={Feng, Sicheng. and Li, Siyu. and Chen*, Luonan. and Chen*, Shengquan.},
  journal={Cell Discovery},
  volume={10},
  pages={122},
  year={2024},
  month={Nov},
  doi={10.1038/s41421-024-00753-1},
  webpage={https://fscdc.github.io/scBackdoor/},
  pdf={https://www.nature.com/articles/s41421-024-00753-1},
  code={https://github.com/BioX-NKU/scBackdoor},
  seqchina={https://mp.weixin.qq.com/s/n-TP8O1zQAy8ITBJewWARg},
  additional_info={ (14.8/Q1 TOP)},
  annotation={* Corresponding authors},
  selected={true},
}