<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T09:27:24+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-20-time-llm/" rel="alternate" type="text/html" title=""/><published>2024-09-02T09:27:24+00:00</published><updated>2024-09-02T09:27:24+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-20-time-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-20-time-llm/"><![CDATA[<h1 id="time-llm-time-series-forecasting-by-reprogramming-large-language-models">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h1> <h2 id="1-ç ”ç©¶èƒŒæ™¯åŠ¨æœº">1. ç ”ç©¶èƒŒæ™¯/åŠ¨æœº</h2> <p>LLM åœ¨ NLP/CV é¢†åŸŸçš„è¡¨ç°éå¸¸ä¼˜å¼‚ï¼Œå…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œåœ¨ zero-shot å’Œ few-shot ä»»åŠ¡ä¸Šä¹Ÿå±•ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ—¶åºé¢„æµ‹(ST)é¢†åŸŸï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½å…·æœ‰è¾ƒå¼ºçš„é’ˆå¯¹æ€§ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚ç ”ç©¶æ˜¾ç¤ºï¼ŒLLM åœ¨æ¨¡å¼è¯†åˆ«å’Œå¤æ‚ token åºåˆ—ç†è§£æ–¹é¢å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚</p> <h2 id="2-åˆ›æ–°ç‚¹">2. åˆ›æ–°ç‚¹</h2> <p>æœ¬ç ”ç©¶åœ¨æ—¶åºé¢„æµ‹é¢†åŸŸé¦–æ¬¡å°è¯•å¼•å…¥ LLMï¼Œé‡‡ç”¨çš„æ˜¯ä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§è°ƒèŠ‚å±‚ï¼Œæ— éœ€ä¿®æ”¹åŸæœ‰ LLM å‚æ•°ï¼Œé¿å…äº†æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚è¿™äº›è°ƒèŠ‚å±‚èƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé€‚åº”å½“å‰ä»»åŠ¡ã€‚å…¶ä¸­ï¼Œæ—¶åºç‰¹å¾çš„ reprogramming æ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå…·æœ‰æ–°é¢–æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p> <h2 id="3-ä¸»è¦æ–¹æ³•">3. ä¸»è¦æ–¹æ³•</h2> <p><img src="/pic/time-llm/structure.jpg" alt="æ¶æ„å›¾"/> æ¶æ„ä¸­çš„ LLM å‚æ•°ä¿æŒä¸å˜ï¼ˆFrozenï¼‰ï¼Œåœ¨å…¶å‰åå„æ·»åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„å±‚ï¼ˆPatch Reprogramming å’Œ Output Projectionï¼‰ã€‚è¿™é‡Œé‡‡ç”¨é€šé“ç‹¬ç«‹ç­–ç•¥ã€‚</p> <h3 id="input-embedding">Input Embedding</h3> <p>å¦‚ä¸Šå›¾åºå· 1 å’Œåºå· 2 æ‰€ç¤ºï¼Œæ—¶é—´åºåˆ—å…ˆé€šè¿‡ RevIN çš„å½’ä¸€åŒ–æ“ä½œï¼Œç„¶ååˆ† patch è¿›è¡Œ embeddingã€‚å…·ä½“æ•°æ®æ ¼å¼å¯è§ï¼š <img src="/pic/time-llm/data.jpg" alt="æ•°æ®æ ¼å¼"/></p> <h3 id="patch-reprogramming">Patch Reprogramming</h3> <p>æ•°æ®é€šè¿‡å‰é¢çš„å¤„ç†åä»ä¸ºæ—¶åºæ•°æ®ï¼Œå› æ­¤éœ€è¦è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ä¾› LLM ä½¿ç”¨ã€‚è¿™é‡Œé‡‡ç”¨äº† cross-attention æ¥å¯¹é½ä¸åŒæ¨¡æ€ï¼Œé€šè¿‡ä¸€ä¸ª linear å±‚ä» V ä¸ªè¯ä¸­æŠ½å– Vâ€™ ä¸ª prototypesï¼Œå‡å°‘äº†å¤„ç†çš„å¤æ‚æ€§ã€‚å…·ä½“æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š <img src="/pic/time-llm/reprogram.jpg" alt="æ¶æ„ç»†èŠ‚"/></p> <h3 id="prompt-as-prefix">Prompt-as-Prefix</h3> <p>å°†æ—¶é—´åºåˆ—æ•°æ®é›†çš„ä¸€äº›å…ˆéªŒä¿¡æ¯ï¼Œä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼ä½œä¸ºå‰ç¼€ promptï¼Œå¹¶ä¸å¯¹é½åçš„æ—¶åºç‰¹å¾æ‹¼æ¥åè¾“å…¥åˆ° LLMã€‚è¿™å¯ä»¥å‚ç…§æ€»ä½“æ¶æ„å›¾çš„åºå· 4ã€‚ä¸€ä¸ªå¯èƒ½çš„ prompt ç¤ºä¾‹ï¼š <img src="/pic/time-llm/prompt.jpg" alt="Prompt ç¤ºä¾‹"/></p> <h3 id="output-projection">Output Projection</h3> <p>ä¸¢å¼ƒå‰ç¼€éƒ¨åˆ†ï¼Œå°†å‰©ä½™éƒ¨åˆ†æ‰å¹³åŒ–å¤„ç†ï¼Œç„¶åé€šè¿‡çº¿æ€§æŠ•å½±æ˜ å°„åˆ°æœ€ç»ˆç»“æœçš„æ ¼å¼ä¸Šã€‚</p> <h2 id="4-æ•°æ®é›†">4. æ•°æ®é›†</h2> <ul> <li>é•¿æœŸï¼šETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI</li> <li>çŸ­æœŸï¼šM4 benchmark</li> </ul> <h2 id="5-å®éªŒç»“æœ">5. å®éªŒç»“æœ</h2> <ul> <li>é•¿æœŸã€çŸ­æœŸ</li> <li>Few-shot on 10%/5%</li> <li>Zero-shot</li> </ul> <p>æ•´ä¸ªæ•ˆæœéƒ½ä¸é”™ï¼Œå…·ä½“æ•°æ®å¯è§åŸè®ºæ–‡çš„Expéƒ¨åˆ†ï¼Œè¿™é‡Œçœç•¥ã€‚</p> <h2 id="6-å¤ç°æƒ…å†µ">6. å¤ç°æƒ…å†µ</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="./Backdoor/BL-survey.md">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="./Backdoor/servey4llmsp.md">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="./Backdoor/BA4llm.md">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="./bio/scGPT.md">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry><entry><title type="html">Efficient LLM</title><link href="https://fscdc.github.io/blog/2024/efficient-llm/" rel="alternate" type="text/html" title="Efficient LLM"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/efficient-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/efficient-llm/"><![CDATA[<p>This Page collects the papers and codes of Efficient AI, Efficient Large Language Models (LLMs). Additional, I read paper and take notes.</p> <p>LIST:</p> <h2 id="-efficient-llm">ğŸ¦™ Efficient LLM</h2> <h3 id="survey">Survey</h3> <ul> <li>Efficient Large Language Models: A Survey, <a href="https://arxiv.org/abs/2312.03863">Arxiv</a>, <a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Github repo</a></li> </ul> <h2 id="books--courses">Books &amp; Courses</h2> <ul> <li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han (I may update some my learning notes later on my <a href="https://fscdc.github.io/">homepage</a>)</li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Efficient LLM]]></summary></entry><entry><title type="html">Interesting Paper</title><link href="https://fscdc.github.io/blog/2024/interesting/" rel="alternate" type="text/html" title="Interesting Paper"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/interesting</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/interesting/"><![CDATA[<p>This Page collects the papers and codes of which attracted my interests. Additional, I read paper and take notes.</p> <p><em>Keyword: effective LLM, multimodal, cross-discipline, Leverage Learning.</em></p> <p>LIST:</p> <ul> <li> <p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces, in <em>arXiv</em> 2023. <a href="https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf">[Paper]</a></p> </li> <li> <p>Token-Efficient Leverage Learning in Large Language Models, in <em>arXiv</em> 2024. <a href="https://arxiv.org/pdf/2404.00914.pdf">[Paper]</a></p> </li> <li> <p>Heterogeneous Graph Neural Network, in <em>ACM</em> 2019 <a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330961">[Paper]</a> <a href="./interesting/het.md">[Note]</a></p> </li> <li> <p>S4 model, <a href="https://arxiv.org/pdf/2111.00396.pdf">[Paper]</a> <a href="./interesting/s4.md">[Note]</a></p> </li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Interesting Paper]]></summary></entry><entry><title type="html">Model for Time Series</title><link href="https://fscdc.github.io/blog/2024/model4ts/" rel="alternate" type="text/html" title="Model for Time Series"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/model4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/model4ts/"><![CDATA[<p>This Page collects the papers and codes of Large Language Models (LLMs) and Foundation Models (FMs) for Time Series (TS). Additional, I read paper and take notes.</p> <p><em>After the success of BERT, GPT, and other LLMs in NLP, some researchers have proposed to apply LLMs to Time Series (TS) tasks. They fintune the LLMs on TS datasets and achieve SOTA results.</em></p> <p>LIST:</p> <h2 id="-llms-for-time-series">ğŸ¦™ LLMs for Time Series</h2> <ul> <li> <p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. <a href="https://arxiv.org/abs/2310.01728">[Paper]</a> <a href="./LLM-TS/time-llm.md">[Note]</a></p> </li> <li> <p>TEST: Text Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series. <a href="https://arxiv.org/abs/2308.08241">[Paper]</a> <a href="./LLM-TS/test.md">[Note]</a></p> </li> <li> <p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2210.08964">[Paper]</a> <a href="./LLM-TS/promptcast.md">[Note]</a></p> </li> <li> <p>One Fits All: Power General Time Series Analysis by Pretrained LM, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2302.11939">[Paper]</a> <a href="./LLM-TS/onefitsall.md">[Note]</a></p> </li> <li> <p>Temporal Data Meets LLM â€“ Explainable Financial Time Series Forecasting, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2306.11025">[Paper]</a></p> </li> <li> <p>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. <a href="https://arxiv.org/abs/2308.08469">[Paper]</a> <a href="./LLM-TS/llm4ts.md">[Note]</a></p> </li> <li> <p>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. <a href="https://arxiv.org/abs/2309.06236">[Paper]</a></p> </li> <li> <p>Large Language Models Are Zero-Shot Time Series Forecasters. <a href="https://arxiv.org/abs/2310.07820">[Paper]</a> <a href="./LLM-TS/llm4zeroshot.md">[Note]</a></p> </li> <li> <p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. <a href="https://arxiv.org/abs/2310.04948">[Paper]</a> <a href="./LLM-TS/tempo.md">[Note]</a></p> </li> <li> <p>S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting. <a href="https://arxiv.org/pdf/2403.05798.pdf">[Paper]</a></p> </li> </ul> <h3 id="-survey">ğŸ“ Survey</h3> <ul> <li> <p>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. <a href="https://arxiv.org/abs/2310.10196">[Survey]</a></p> </li> <li> <p>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. <a href="https://arxiv.org/abs/2402.02713">[Survey]</a></p> </li> <li> <p>Foundation Models for Time Series Analysis: A Tutorial and Survey <a href="https://arxiv.org/abs/2403.14735">[Survey]</a></p> </li> </ul> <h2 id="-related-fields">ğŸ”— Related Fields</h2> <p><em>Here, some related fields are listed. These fields are not the main focus of this project, but they are also important for understanding how LLMs are applied to other fields rather than NLP and FMs in specific fields are developed.</em></p> <h3 id="-llm-for-recommendation-systems">ğŸ“ LLM for Recommendation Systems</h3> <ul> <li>Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5), in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2203.13366">[Paper]</a></li> <li>LLM4Rec. <a href="https://github.com/WLiK/LLM4Rec">[GitHub]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Model for Time Series]]></summary></entry><entry><title type="html">Paper Notes</title><link href="https://fscdc.github.io/blog/2024/PaperNotes/" rel="alternate" type="text/html" title="Paper Notes"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://fscdc.github.io/blog/2024/PaperNotes</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/PaperNotes/"><![CDATA[<p>This post is for update my papers notes. Below are my research notes from papers in various fields:</p> <ol> <li>Please click <a href="https://fscdc.github.io/blog/2024/model4ts/">HERE</a>(LLM-based Time series analysis) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/backdoor/">HERE</a>(Backdoor attacks) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/interesting/">HERE</a>(Interesting papers) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/bio/">HERE</a>(papers about AI4Bio, scRNA-seq. etc.) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/efficient-llm/">HERE</a>(papers about Efficient AI, Efficient LLM, etc.) for more details. </li> <li>Please click <a href="">HERE</a>(papers about Pruning, not ready for now) for more details.</li> </ol> ]]></content><author><name></name></author><category term="summary"/><category term="note"/><summary type="html"><![CDATA[this is my notes for papers]]></summary></entry></feed>