<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T09:19:03+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="./Backdoor/BL-survey.md">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="./Backdoor/servey4llmsp.md">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="./Backdoor/BA4llm.md">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="./bio/scGPT.md">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry><entry><title type="html">Efficient LLM</title><link href="https://fscdc.github.io/blog/2024/efficient-llm/" rel="alternate" type="text/html" title="Efficient LLM"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/efficient-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/efficient-llm/"><![CDATA[<p>This Page collects the papers and codes of Efficient AI, Efficient Large Language Models (LLMs). Additional, I read paper and take notes.</p> <p>LIST:</p> <h2 id="-efficient-llm">ü¶ô Efficient LLM</h2> <h3 id="survey">Survey</h3> <ul> <li>Efficient Large Language Models: A Survey, <a href="https://arxiv.org/abs/2312.03863">Arxiv</a>, <a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Github repo</a></li> </ul> <h2 id="books--courses">Books &amp; Courses</h2> <ul> <li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han (I may update some my learning notes later on my <a href="https://fscdc.github.io/">homepage</a>)</li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Efficient LLM]]></summary></entry><entry><title type="html">Interesting Paper</title><link href="https://fscdc.github.io/blog/2024/interesting/" rel="alternate" type="text/html" title="Interesting Paper"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/interesting</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/interesting/"><![CDATA[<p>This Page collects the papers and codes of which attracted my interests. Additional, I read paper and take notes.</p> <p><em>Keyword: effective LLM, multimodal, cross-discipline, Leverage Learning.</em></p> <p>LIST:</p> <ul> <li> <p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces, in <em>arXiv</em> 2023. <a href="https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf">[Paper]</a></p> </li> <li> <p>Token-Efficient Leverage Learning in Large Language Models, in <em>arXiv</em> 2024. <a href="https://arxiv.org/pdf/2404.00914.pdf">[Paper]</a></p> </li> <li> <p>Heterogeneous Graph Neural Network, in <em>ACM</em> 2019 <a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330961">[Paper]</a> <a href="./interesting/het.md">[Note]</a></p> </li> <li> <p>S4 model, <a href="https://arxiv.org/pdf/2111.00396.pdf">[Paper]</a> <a href="./interesting/s4.md">[Note]</a></p> </li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Interesting Paper]]></summary></entry><entry><title type="html">Model for Time Series</title><link href="https://fscdc.github.io/blog/2024/model4ts/" rel="alternate" type="text/html" title="Model for Time Series"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/model4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/model4ts/"><![CDATA[<p>This Page collects the papers and codes of Large Language Models (LLMs) and Foundation Models (FMs) for Time Series (TS). Additional, I read paper and take notes.</p> <p><em>After the success of BERT, GPT, and other LLMs in NLP, some researchers have proposed to apply LLMs to Time Series (TS) tasks. They fintune the LLMs on TS datasets and achieve SOTA results.</em></p> <p>LIST:</p> <h2 id="-llms-for-time-series">ü¶ô LLMs for Time Series</h2> <ul> <li> <p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. <a href="https://arxiv.org/abs/2310.01728">[Paper]</a> <a href="./LLM-TS/time-llm.md">[Note]</a></p> </li> <li> <p>TEST: Text Prototype Aligned Embedding to Activate LLM‚Äôs Ability for Time Series. <a href="https://arxiv.org/abs/2308.08241">[Paper]</a> <a href="./LLM-TS/test.md">[Note]</a></p> </li> <li> <p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2210.08964">[Paper]</a> <a href="./LLM-TS/promptcast.md">[Note]</a></p> </li> <li> <p>One Fits All: Power General Time Series Analysis by Pretrained LM, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2302.11939">[Paper]</a> <a href="./LLM-TS/onefitsall.md">[Note]</a></p> </li> <li> <p>Temporal Data Meets LLM ‚Äì Explainable Financial Time Series Forecasting, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2306.11025">[Paper]</a></p> </li> <li> <p>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. <a href="https://arxiv.org/abs/2308.08469">[Paper]</a> <a href="./LLM-TS/llm4ts.md">[Note]</a></p> </li> <li> <p>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. <a href="https://arxiv.org/abs/2309.06236">[Paper]</a></p> </li> <li> <p>Large Language Models Are Zero-Shot Time Series Forecasters. <a href="https://arxiv.org/abs/2310.07820">[Paper]</a> <a href="./LLM-TS/llm4zeroshot.md">[Note]</a></p> </li> <li> <p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. <a href="https://arxiv.org/abs/2310.04948">[Paper]</a> <a href="./LLM-TS/tempo.md">[Note]</a></p> </li> <li> <p>S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting. <a href="https://arxiv.org/pdf/2403.05798.pdf">[Paper]</a></p> </li> </ul> <h3 id="-survey">üìç Survey</h3> <ul> <li> <p>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. <a href="https://arxiv.org/abs/2310.10196">[Survey]</a></p> </li> <li> <p>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. <a href="https://arxiv.org/abs/2402.02713">[Survey]</a></p> </li> <li> <p>Foundation Models for Time Series Analysis: A Tutorial and Survey <a href="https://arxiv.org/abs/2403.14735">[Survey]</a></p> </li> </ul> <h2 id="-related-fields">üîó Related Fields</h2> <p><em>Here, some related fields are listed. These fields are not the main focus of this project, but they are also important for understanding how LLMs are applied to other fields rather than NLP and FMs in specific fields are developed.</em></p> <h3 id="-llm-for-recommendation-systems">üìç LLM for Recommendation Systems</h3> <ul> <li>Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5), in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2203.13366">[Paper]</a></li> <li>LLM4Rec. <a href="https://github.com/WLiK/LLM4Rec">[GitHub]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Model for Time Series]]></summary></entry><entry><title type="html">Paper Notes</title><link href="https://fscdc.github.io/blog/2024/PaperNotes/" rel="alternate" type="text/html" title="Paper Notes"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://fscdc.github.io/blog/2024/PaperNotes</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/PaperNotes/"><![CDATA[<p>This post is for update my papers notes. Below are my research notes from papers in various fields:</p> <ol> <li>Please click <a href="https://fscdc.github.io/blog/2024/model4ts/">HERE</a>(LLM-based Time series analysis) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/backdoor/">HERE</a>(Backdoor attacks) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/interesting/">HERE</a>(Interesting papers) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/bio/">HERE</a>(papers about AI4Bio, scRNA-seq. etc.) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/efficient-llm/">HERE</a>(papers about Efficient AI, Efficient LLM, etc.) for more details. </li> <li>Please click <a href="">HERE</a>(papers about Pruning, not ready for now) for more details.</li> </ol> ]]></content><author><name></name></author><category term="summary"/><category term="note"/><summary type="html"><![CDATA[this is my notes for papers]]></summary></entry></feed>