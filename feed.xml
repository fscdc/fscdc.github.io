<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2023-05-18-llm4ts/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2023-05-18-llm4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2023-05-18-llm4ts/"><![CDATA[<h1 id="llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms">LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs</h1> <h2 id="method">Method</h2> <p><img src="/assets/img/pic/llm4ts/structure.jpg" alt="Overall Architecture"/></p> <h2 id="time-series-alignment">Time-Series Alignment</h2> <p>Perform autoregressive processing on time-series data to analyze the relationship between inputs and outputs.</p> <h3 id="instance-normalization">Instance Normalization</h3> <p>Normalize the entire data instance, which is part of the data preprocessing.</p> <h3 id="time-series-tokenization">Time-Series Tokenization</h3> <p>A typical data processing method using channel-independent strategies combined with patching. <img src="/assets/img/pic/llm4ts/patch.jpg" alt="Time-Series Tokenization Diagram"/></p> <h3 id="three-encodings-for-patched-time-series-data">Three Encodings for Patched Time-Series Data</h3> <ul> <li><strong>Token Embedding Layer</strong>: Use a 1D convolutional layer as the token embedding layer.</li> <li><strong>Positional Layer</strong>: Use a trainable lookup table to identify the position of each patch.</li> <li><strong>Temporal Embedding Layer</strong>: Achieve dual-level aggregation. <ul> <li>First, use a trainable lookup table for each temporal attribute (e.g., second, minute) to map each attribute to a high-dimensional space, then sum these mappings to form a unified temporal embedding representation.</li> <li>Each patch may contain multiple timestamps, and pooling methods are used to extract the final temporal embedding, usually by taking the first timestamp in the patch as the representative of the entire patch.</li> </ul> </li> </ul> <h3 id="partial-freezing-and-tunable-layers">Partial Freezing and Tunable Layers</h3> <p>Freeze certain parts of the model while keeping some layers trainable to optimize model performance.</p> <ul> <li><strong>Layer Normalization Tuning</strong></li> <li><strong>Low-Rank Adaptation (LoRA)</strong></li> </ul> <h2 id="forecasting-fine-tuning-strategy">Forecasting Fine-Tuning Strategy</h2> <p>Adopt the initial linear probing followed by full fine-tuning (LP-FT) strategy, whose superiority lies in its dual-phase approach:</p> <ul> <li>First, optimize the output layer to minimize adjustments during fine-tuning (preserving the effectiveness of the feature extractor in OOD (Out-Of-Distribution) scenarios).</li> <li>Then conduct full fine-tuning to adapt the model to specific tasks (improving ID (In-Distribution) accuracy).</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-onefitsall/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-onefitsall</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-onefitsall/"><![CDATA[<h1 id="one-fits-all-power-general-time-series-analysis-by-pretrained-lm">One Fits All: Power General Time Series Analysis by Pretrained LM</h1> <h2 id="1-ç ”ç©¶èƒŒæ™¯åŠ¨æœº">1. ç ”ç©¶èƒŒæ™¯/åŠ¨æœº</h2> <p>ç•¥</p> <h2 id="2-åˆ›æ–°ç‚¹">2. åˆ›æ–°ç‚¹</h2> <p>ä»»åŠ¡å…¨é¢ï¼Œæ–¹æ³•è™½ç„¶ç›¸å¯¹ç®€å•ï¼Œä½†å¾®è°ƒæ•ˆæœè‰¯å¥½ã€‚</p> <h2 id="3-ä¸»è¦æ–¹æ³•">3. ä¸»è¦æ–¹æ³•</h2> <p><img src="/assets/img/pic/onefitsall/structure.jpg" alt="æ–¹æ³•æ¦‚è¿°å›¾"/> æœ¬ç ”ç©¶çš„æ–¹æ³•ç®€å•ç›´æ¥ï¼š</p> <ul> <li><strong>Self-attention</strong> å’Œ <strong>FFN</strong> è¢«å†»ç»“ï¼Œå³LLMçš„æ ¸å¿ƒéƒ¨åˆ†è¢«å†»ç»“ã€‚åªè®­ç»ƒ <strong>positional embedding</strong>ã€<strong>input embedding</strong>ã€<strong>çº¿æ€§è¾“å‡ºå±‚</strong> å’Œ <strong>Layer Normå±‚</strong>ã€‚</li> <li>å…¶ä¸­ <strong>positional embedding</strong> å’Œ <strong>layer norm</strong> éœ€è¦é’ˆå¯¹ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯å¾ˆè‡ªç„¶çš„è¿‡ç¨‹ã€‚</li> <li><strong>Input embedding</strong> æ˜¯å¿…é¡»è¿›è¡Œçš„æ­¥éª¤ï¼Œè¿™é‡Œåˆ©ç”¨çš„æŠ€æœ¯æ˜¯ <strong>linear probing</strong>ï¼ˆå‚æ•°è¾ƒå°‘ï¼‰ã€‚</li> <li>è¿›è¡Œç®€å•çš„å‡å€¼-æ–¹å·®å½’ä¸€åŒ–ã€‚</li> <li><strong>Patching</strong>ï¼Œå³èšåˆç›¸é‚»æ—¶é—´æ­¥æ¥å½¢æˆä¸€ä¸ªtokenï¼Œåœ¨åŒæ ·çš„è¾“å…¥é•¿åº¦ä¸‹ï¼Œè¿™æ ·å¯ä»¥è¦†ç›–æ›´å¤§è·¨åº¦çš„æ—¶é—´èŒƒå›´ã€‚</li> </ul> <h3 id="connecting-self-attention-with-pca">Connecting Self-Attention with PCA</h3> <p>ä½œè€…åœ¨æ–‡ç« ä¸­è¿˜è¯æ˜äº† <strong>self-attention</strong> å’Œ <strong>PCA</strong> åœ¨ä½œç”¨ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œå…·ä½“è¯æ˜çœç•¥ã€‚ä»å®éªŒæ¥çœ‹ï¼ŒäºŒè€…åœ¨åŠŸèƒ½ä¸Šæ˜¾ç¤ºå‡ºä¸€å®šçš„ç›¸ä¼¼æ€§ï¼Œè¿™å¼ºè°ƒäº†é¢„è®­ç»ƒå¥½çš„ self-attention å¯¹äºå»ºæ¨¡å„ç§æ¨¡æ€æ•°æ®çš„é€šç”¨æ€§ã€‚</p> <h2 id="4-æ•°æ®é›†">4. æ•°æ®é›†</h2> <p>ä½¿ç”¨äº†å¤šç§æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„ä»»åŠ¡æµ‹è¯•ï¼Œè¯¦è§åŸæ–‡ã€‚</p> <h2 id="5-å®éªŒç»“æœ">5. å®éªŒç»“æœ</h2> <ul> <li>é•¿æœŸé¢„æµ‹ï¼šé¢„æµ‹é•¿åº¦æ›´é•¿</li> <li>çŸ­æœŸé¢„æµ‹ï¼šé¢„æµ‹é•¿åº¦è¾ƒçŸ­</li> <li>é›¶æ ·æœ¬é¢„æµ‹ï¼šæœªè¿›è¡Œå¾®è°ƒ</li> <li>å°‘æ ·æœ¬é¢„æµ‹ï¼šåªç”¨æå°‘é‡çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œå¾®è°ƒ</li> <li>åˆ†ç±»</li> <li>å¼‚å¸¸æ£€æµ‹</li> <li>æ’è¡¥</li> </ul> <p>ä»¥ä¸Šä¸ƒé¡¹ä»»åŠ¡å‡è¿›è¡Œäº†æµ‹è¯•ã€‚</p> <h2 id="6-å®éªŒç¯å¢ƒ">6. å®éªŒç¯å¢ƒ</h2> <p>ä½œè€…è®ºæ–‡ä¸­è¯¦ç»†åˆ—å‡ºäº†è®¡ç®—æˆæœ¬ã€‚</p> <h2 id="7-å¤ç°">7. å¤ç°</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-promptcast/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-promptcast</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-promptcast/"><![CDATA[<h1 id="promptcast-a-new-prompt-based-learning-paradigm-for-time-series-forecasting">PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting</h1> <p><img src="/assets/img/pic/promptcast/structure.jpg" alt="PromptCast æ¨¡å‹æ¶æ„å›¾"/></p> <p>è¿™ç¯‡æ–¹æ³•å¤ªå¹³å‡¡äº†ï¼Œç®€å•è¯´ä¸‹ï¼š</p> <h2 id="ç ”ç©¶æ¦‚è¿°">ç ”ç©¶æ¦‚è¿°</h2> <p>è¿™ç¯‡è®ºæ–‡å‘è¡¨äº2022å¹´ï¼Œå°è¯•é¦–æ¬¡å°†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åº”ç”¨äºæ—¶é—´åºåˆ—ï¼ˆTSï¼‰é¢„æµ‹ä»»åŠ¡ã€‚å°½ç®¡æ–¹æ³•æœ¬èº«ç›¸å¯¹åŸºç¡€ï¼Œä½†è¿™æ ‡å¿—ç€åœ¨è¯¥é¢†åŸŸçš„ä¸€æ¬¡æ–°çš„å°è¯•ã€‚</p> <h2 id="ä¸»è¦æ–¹æ³•">ä¸»è¦æ–¹æ³•</h2> <p>ä½œè€…é‡‡ç”¨äº†ä¸€ä¸ªç›´æ¥è€Œç®€å•çš„ç­–ç•¥æ¥å®æ–½è¿™ä¸€æ–°çš„å­¦ä¹ èŒƒå¼ã€‚æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š</p> <ul> <li><strong>Promptè®¾è®¡</strong>ï¼šé€šè¿‡ç®€å•çš„è®¾è®¡ä¿®æ”¹ï¼Œå°†æ—¶é—´åºåˆ—æ•°æ®ä»¥é€‚åˆè¯­è¨€æ¨¡å‹å¤„ç†çš„æ–¹å¼å‘ˆç°ã€‚</li> <li><strong>ç›´æ¥è¯¢é—®</strong>ï¼šç±»ä¼¼äºæé—®å¼å­¦ä¹ ï¼Œç›´æ¥å‘æ¨¡å‹è¯¢é—®æœªæ¥çš„æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œè€Œä¸è¿›è¡Œå¤æ‚çš„è½¬æ¢æˆ–é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤ã€‚</li> </ul> <p><img src="/assets/img/pic/promptcast/prompt.jpg" alt="PromptCast æ–¹æ³•ç¤ºæ„å›¾"/></p> <p>è¿™ç§æ–¹æ³•çš„ç®€ä¾¿æ€§ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸæä¾›äº†ä¸€ç§å…¨æ–°çš„é€”å¾„ï¼Œè™½ç„¶å®ƒçš„æŠ€æœ¯æ·±åº¦å’Œåˆ›æ–°æ€§å¯èƒ½ä¸å¦‚å…¶ä»–æ›´å¤æ‚çš„æ–¹æ³•ï¼Œä½†åœ¨ç®€åŒ–æµç¨‹å’Œç›´æ¥æ€§æ–¹é¢å…·æœ‰å…¶ç‹¬ç‰¹çš„ä»·å€¼ã€‚</p> <h2 id="ç»“è®º">ç»“è®º</h2> <p>å°½ç®¡â€œPromptCastâ€çš„æ–¹æ³•å¯èƒ½çœ‹èµ·æ¥è¾ƒä¸ºå¹³å‡¡ï¼Œä½†å®ƒå¼€åˆ›äº†åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹çš„æ–°é¢†åŸŸï¼Œä¸ºåç»­çš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-tempo/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-tempo</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-tempo/"><![CDATA[<h1 id="tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting">TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting</h1> <h2 id="1-ç ”ç©¶èƒŒæ™¯åŠ¨æœº">1. ç ”ç©¶èƒŒæ™¯/åŠ¨æœº</h2> <p>ç•¥</p> <h2 id="2-åˆ›æ–°ç‚¹">2. åˆ›æ–°ç‚¹</h2> <h2 id="3-ä¸»è¦æ–¹æ³•">3. ä¸»è¦æ–¹æ³•</h2> <p><img src="/assets/img/pic/tempo/structure.jpg" alt="TEMPOæ¨¡å‹æ¶æ„å›¾"/> æœ¬æ–‡é‡‡ç”¨GPTä½œä¸ºbackboneè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼Œå¼•å…¥æç¤ºå·¥ç¨‹å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°é€‚é…æ¥è‡ªä¸åŒé¢†åŸŸçš„éå¹³ç¨³æ—¶åºæ•°æ®ã€‚æ—¶é—´åºåˆ—è¢«è§£è€¦ä¸ºè¶‹åŠ¿é¡¹ã€å­£èŠ‚é¡¹å’Œæ®‹å·®é¡¹ï¼Œå¹¶æ˜ å°„åˆ°ç›¸åº”çš„éšè—ç©ºé—´ï¼Œæ„å»ºGPTèƒ½å¤Ÿè¯†åˆ«çš„è¾“å…¥ã€‚</p> <h3 id="æ—¶é—´åºåˆ—è§£è€¦">æ—¶é—´åºåˆ—è§£è€¦</h3> <ul> <li><strong>è¶‹åŠ¿é¡¹</strong>ï¼šé€šè¿‡è®¡ç®—æŒ‡å®šæ»‘çª—å†…çš„å‡å€¼å¾—åˆ°ã€‚</li> <li><strong>å‘¨æœŸé¡¹</strong>ï¼šåŸåºåˆ—å‡å»è¶‹åŠ¿é¡¹åï¼Œä½¿ç”¨Loess smootherè®¡ç®—å¾—åˆ°ã€‚</li> <li><strong>æ®‹å·®é¡¹</strong>ï¼šåŸåºåˆ—å‡å»è¶‹åŠ¿é¡¹å’Œå‘¨æœŸé¡¹åå¾—åˆ°ã€‚</li> </ul> <h3 id="æ¨¡å‹å»ºæ¨¡æµç¨‹">æ¨¡å‹å»ºæ¨¡æµç¨‹</h3> <p>ä½¿ç”¨patchTSTæ¶æ„ï¼Œé¦–å…ˆå¯¹å‘¨æœŸé¡¹è¿›è¡Œinstance normalizationï¼Œç„¶åè¿›è¡Œpatchingï¼Œéšåé€å…¥embeddingè¿›è¡Œç¼–ç ã€‚</p> <p><img src="/assets/img/pic/tempo/patchtst.jpg" alt="patchTSTæ¶æ„å›¾"/></p> <h3 id="promptè®¾è®¡">Promptè®¾è®¡</h3> <p>æ„å»ºäº†ä¸€ä¸ªpromptæ± ï¼Œæ± ä¸­ä¿ç•™ä¸åŒé”®å€¼å¯¹ï¼Œç›¸ä¼¼çš„è¾“å…¥æ—¶é—´åºåˆ—å€¾å‘äºä»é›†åˆä¸­æ£€ç´¢å‡ºåŒä¸€ç»„æç¤ºè¯ã€‚é‡‡ç”¨å¾—åˆ†åŒ¹é…æœºåˆ¶ï¼Œå°†åŸå§‹åºåˆ—çš„embeddingä¸é”®å€¼å¯¹ä¸­çš„é”®è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œé€‰æ‹©kä¸ªæœ€åŒ¹é…çš„é”®ï¼Œä¸åŸå§‹åºåˆ—çš„embeddingæ‹¼æ¥å½¢æˆæœ€ç»ˆè¾“å…¥ã€‚</p> <h3 id="gpté¢„æµ‹æ¨¡å‹">GPTé¢„æµ‹æ¨¡å‹</h3> <p>ä½¿ç”¨é¢„è®­ç»ƒçš„GPT-2è¿›è¡Œé¢„æµ‹ï¼Œæœ‰ä¸¤ç§æ¨¡å‹å½¢å¼ï¼šä¸€ä¸ªæ˜¯å°†å¤„ç†åçš„è¶‹åŠ¿é¡¹ã€å‘¨æœŸé¡¹å’Œæ®‹å·®é¡¹æ‹¼æ¥åè¾“å…¥GPTï¼›å¦ä¸€ä¸ªæ˜¯ç‹¬ç«‹åœ°æ¨¡å‹è¶‹åŠ¿é¡¹ã€å‘¨æœŸé¡¹å’Œæ®‹å·®é¡¹ã€‚GPTå—å†…éƒ¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“äº†å‰é¦ˆå±‚ï¼Œåªæ›´æ–°ä½ç½®åµŒå…¥å±‚å’Œå±‚å½’ä¸€åŒ–å±‚çš„æ¢¯åº¦ã€‚é‡‡ç”¨LORAè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ä¸åŒçš„æ—¶é—´åºåˆ—åˆ†å¸ƒã€‚è¾“å‡ºç»“æœé€šè¿‡åå½’ä¸€åŒ–åç´¯åŠ ä»¥å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹å€¼ã€‚</p> <h2 id="4-æ•°æ®é›†">4. æ•°æ®é›†</h2> <p>è¿˜æ˜¯å¾ˆç»å…¸çš„é‚£å‡ ä¸ª</p> <h2 id="5-å®éªŒç»“æœ">5. å®éªŒç»“æœ</h2> <p>æ•ˆæœè¿˜ä¸é”™ï¼Œå…·ä½“è§åŸæ–‡ã€‚</p> <h2 id="6-å®éªŒç¯å¢ƒ">6. å®éªŒç¯å¢ƒ</h2> <p>æœªå¼€æºï¼Œè¿™é‡Œæ–¹æ³•ç®€å•æœªå¤ç°</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-test/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-test</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-test/"><![CDATA[<h1 id="test-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series">TEST: Text Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series</h1> <h2 id="1-ç ”ç©¶èƒŒæ™¯åŠ¨æœº">1. ç ”ç©¶èƒŒæ™¯/åŠ¨æœº</h2> <p>åœ¨LLM+TSçš„èƒŒæ™¯ä¸‹ï¼Œå­˜åœ¨ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šLLM-for-TS è®¾è®¡ï¼Œå³è®­ç»ƒä¸€ä¸ªå¤§å‹æ¨¡å‹å¹¶é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼›TS-for-LLMï¼Œå³å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºæ¨¡å‹å‹å¥½çš„è¡¨ç¤ºæ ¼å¼ï¼Œä½¿é¢„è®­ç»ƒçš„LLMå¯ä»¥å¤„ç†æ—¶é—´åºåˆ—ã€‚ç”±äºè½»é‡çº§ç­‰é™åˆ¶ï¼Œæœ¬ç ”ç©¶ä¾ç„¶èšç„¦äº TS-for-LLM ç­–ç•¥ã€‚</p> <h2 id="2-åˆ›æ–°ç‚¹">2. åˆ›æ–°ç‚¹</h2> <p>æœ¬ç ”ç©¶æ²¡æœ‰ä½¿ç”¨é€šé“ç‹¬ç«‹ç­–ç•¥ï¼Œä¿ç•™äº†å¤šå˜é‡æ—¶é—´åºåˆ—ï¼ˆMTSï¼‰çš„ä¿¡æ¯ã€‚å¼•å…¥è½¯æç¤ºï¼ˆsoft promptï¼‰é¿å…äº†å¾®è°ƒæ‰€éœ€çš„é«˜æ˜‚è®­ç»ƒæˆæœ¬ã€‚å¯¹é½åµŒå…¥æ–¹æ³•ä¹Ÿæ˜¯ä¸€å¤§äº®ç‚¹ã€‚</p> <h2 id="3-ä¸»è¦æ–¹æ³•">3. ä¸»è¦æ–¹æ³•</h2> <p>æœ¬æ–‡æ¨¡å‹ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼šæ„å»ºç¼–ç å™¨æ¥åµŒå…¥TSå’Œåˆ›å»ºå¯ä»¥è®©LLMæ¥æ”¶åµŒå…¥çš„TSçš„promptã€‚</p> <h3 id="ç¬¬ä¸€æ­¥">ç¬¬ä¸€æ­¥</h3> <p><img src="/assets/img/pic/test/1.jpg" alt="ç¤ºæ„å›¾"/> é¦–å…ˆä½¿ç”¨ç»å…¸çš„æ»‘åŠ¨çª—å£æ–¹æ³•å°†TS tokenizeï¼Œå³åˆ†æˆé•¿åº¦ä¸å®šçš„å­åºåˆ—ï¼Œæ¯ä¸ªå­åºåˆ—æ˜¯ä¸€ä¸ªæ ‡è®° sï¼ˆanchor instanceï¼‰ã€‚ç„¶åå¯¹æ¯ä¸ªæ ‡è®° sï¼Œé‡‡ç”¨å¼±å¢å¼ºï¼ˆjitter-and-scale strategyï¼‰å’Œå¼ºå¢å¼ºï¼ˆpermutation-and-jitter strategyï¼‰ç”Ÿæˆæ­£ä¾‹ï¼Œå¹¶é€‰å–ä¸ s æœ¬èº«æ— é‡å çš„å®ä¾‹ä½œä¸ºè´Ÿä¾‹ã€‚æ„å»ºç¥ç»ç½‘ç»œä½œä¸ºç¼–ç å™¨å°† anchor instance åµŒå…¥æˆä¸€ä¸ª M ç»´çš„åµŒå…¥å‘é‡ eï¼Œè¿™äº› e å°±æ˜¯æœ€åˆMTSçš„åµŒå…¥token listã€‚</p> <p>æ¥ä¸‹æ¥æ˜¯å¯¹æ¯”å­¦ä¹ éƒ¨åˆ†ï¼š</p> <ul> <li><strong>Instance-wise contrast</strong>ï¼šä¿è¯ç›®æ ‡anchor instanceä¸å…¶å¯¹åº”çš„æ­£token instanceå°½å¯èƒ½ç›¸ä¼¼ï¼Œä¸è´Ÿtoken instanceå·®å¼‚å°½å¯èƒ½å¤§ã€‚ä½¿ç”¨äº†ç‰¹å®šçš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå¹¶é‡‡å–äº†é˜²æ­¢åµŒå…¥ç©ºé—´åå¡Œçš„ç­–ç•¥ã€‚</li> <li><strong>Feature-wise contrast</strong>ï¼šæ‰“ç ´å®ä¾‹ä¹‹é—´çš„ç‹¬ç«‹æ€§ã€‚åœ¨åµŒå…¥åï¼Œæ¯ä¸ªminibatchä¸­ä¼šæœ‰ä¸€ä¸ªç”±Bä¸ªinstanceç»„æˆçš„ç‰¹å¾çŸ©é˜µBÃ—Mï¼Œèšç±»æœªçŸ¥ï¼Œå°†åˆ—è§†ä¸ºç‰¹å¾çš„è½¯æ ‡ç­¾ï¼Œå¹¶å¯¹ç›¸ä¼¼ç‰¹å¾çš„ç»„è¿›è¡ŒåŒºåˆ†ã€‚</li> <li><strong>Text-prototype-aligned contrast learning</strong>ï¼šå°†æ—¶åºåµŒå…¥å‘é‡å¯¹é½åˆ°LLMçš„æ–‡æœ¬è¡¨ç¤ºç©ºé—´ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œçº¦æŸå‘é‡çš„ç›¸ä¼¼æ€§å’Œç›¸ä¼¼çš„å®ä¾‹åœ¨æ–‡æœ¬ç©ºé—´ä¹Ÿæœ‰ç±»ä¼¼çš„è¡¨ç¤ºã€‚</li> </ul> <h3 id="ç¬¬äºŒæ­¥soft-prompt">ç¬¬äºŒæ­¥ï¼šSoft Prompt</h3> <p>è¿™äº›soft promptæ˜¯é’ˆå¯¹ä¸‹æ¸¸å…·ä½“ä»»åŠ¡çš„åµŒå…¥ï¼Œé€šè¿‡LLMè¾“å‡ºå’Œground truthä¹‹é—´çš„lossæ¥å­¦ä¹ ã€‚æ³¨æ„ï¼Œè¿™é‡Œçš„promptä¸å†æ˜¯äººç±»è¯­ä¹‰ã€‚</p> <p><img src="/assets/img/pic/test/structure.jpg" alt="æ•´ä½“æ¶æ„å›¾"/></p> <h2 id="4-æ•°æ®é›†">4. æ•°æ®é›†</h2> <ul> <li>åˆ†ç±»ï¼šUCR archiveä¸­çš„æ‰€æœ‰128ç§å•å˜é‡æ—¶é—´åºåˆ—æ•°æ®é›†</li> <li>é¢„æµ‹ï¼šåŒ…æ‹¬å¤©æ°”ã€äº¤é€šã€ç”µåŠ›ã€ILI å’Œ ETT ç­‰8ä¸ªæµè¡Œçš„å®é™…åº”ç”¨åŸºå‡†æ•°æ®é›†</li> </ul> <h2 id="5-å®éªŒç»“æœ">5. å®éªŒç»“æœ</h2> <p>æ—¶åºåˆ†ç±»å’Œé¢„æµ‹æ€§èƒ½ä¸å¸¸è§çš„baselineç›¸è¿‘ã€‚</p> <h2 id="6-å®éªŒç¯å¢ƒ">6. å®éªŒç¯å¢ƒ</h2> <p>20 NVIDIA Tesla V100-SXM2 GPU with CUDA 11.3.</p> <h2 id="7-å¤ç°æƒ…å†µ">7. å¤ç°æƒ…å†µ</h2> <p>æœ¬æœºç¯å¢ƒå·®è·‘ä¸èµ·æ¥ï¼Œä½†æ˜¯modelçœ‹å®Œäº†</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-time-llm/" rel="alternate" type="text/html" title=""/><published>2024-09-04T14:59:14+00:00</published><updated>2024-09-04T14:59:14+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-time-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-time-llm/"><![CDATA[<h1 id="time-llm-time-series-forecasting-by-reprogramming-large-language-models">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h1> <h2 id="1-ç ”ç©¶èƒŒæ™¯åŠ¨æœº">1. ç ”ç©¶èƒŒæ™¯/åŠ¨æœº</h2> <p>LLM åœ¨ NLP/CV é¢†åŸŸçš„è¡¨ç°éå¸¸ä¼˜å¼‚ï¼Œå…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œåœ¨ zero-shot å’Œ few-shot ä»»åŠ¡ä¸Šä¹Ÿå±•ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ—¶åºé¢„æµ‹(ST)é¢†åŸŸï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½å…·æœ‰è¾ƒå¼ºçš„é’ˆå¯¹æ€§ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚ç ”ç©¶æ˜¾ç¤ºï¼ŒLLM åœ¨æ¨¡å¼è¯†åˆ«å’Œå¤æ‚ token åºåˆ—ç†è§£æ–¹é¢å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚</p> <h2 id="2-åˆ›æ–°ç‚¹">2. åˆ›æ–°ç‚¹</h2> <p>æœ¬ç ”ç©¶åœ¨æ—¶åºé¢„æµ‹é¢†åŸŸé¦–æ¬¡å°è¯•å¼•å…¥ LLMï¼Œé‡‡ç”¨çš„æ˜¯ä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§è°ƒèŠ‚å±‚ï¼Œæ— éœ€ä¿®æ”¹åŸæœ‰ LLM å‚æ•°ï¼Œé¿å…äº†æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚è¿™äº›è°ƒèŠ‚å±‚èƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé€‚åº”å½“å‰ä»»åŠ¡ã€‚å…¶ä¸­ï¼Œæ—¶åºç‰¹å¾çš„ reprogramming æ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå…·æœ‰æ–°é¢–æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p> <h2 id="3-ä¸»è¦æ–¹æ³•">3. ä¸»è¦æ–¹æ³•</h2> <p><img src="/assets/img/pic/time-llm/structure.jpg" alt="æ¶æ„å›¾"/> æ¶æ„ä¸­çš„ LLM å‚æ•°ä¿æŒä¸å˜ï¼ˆFrozenï¼‰ï¼Œåœ¨å…¶å‰åå„æ·»åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„å±‚ï¼ˆPatch Reprogramming å’Œ Output Projectionï¼‰ã€‚è¿™é‡Œé‡‡ç”¨é€šé“ç‹¬ç«‹ç­–ç•¥ã€‚</p> <h3 id="input-embedding">Input Embedding</h3> <p>å¦‚ä¸Šå›¾åºå· 1 å’Œåºå· 2 æ‰€ç¤ºï¼Œæ—¶é—´åºåˆ—å…ˆé€šè¿‡ RevIN çš„å½’ä¸€åŒ–æ“ä½œï¼Œç„¶ååˆ† patch è¿›è¡Œ embeddingã€‚å…·ä½“æ•°æ®æ ¼å¼å¯è§ï¼š <img src="/assets/img/pic/time-llm/data.jpg" alt="æ•°æ®æ ¼å¼"/></p> <h3 id="patch-reprogramming">Patch Reprogramming</h3> <p>æ•°æ®é€šè¿‡å‰é¢çš„å¤„ç†åä»ä¸ºæ—¶åºæ•°æ®ï¼Œå› æ­¤éœ€è¦è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ä¾› LLM ä½¿ç”¨ã€‚è¿™é‡Œé‡‡ç”¨äº† cross-attention æ¥å¯¹é½ä¸åŒæ¨¡æ€ï¼Œé€šè¿‡ä¸€ä¸ª linear å±‚ä» V ä¸ªè¯ä¸­æŠ½å– Vâ€™ ä¸ª prototypesï¼Œå‡å°‘äº†å¤„ç†çš„å¤æ‚æ€§ã€‚å…·ä½“æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š <img src="/assets/img/pic/time-llm/reprogram.jpg" alt="æ¶æ„ç»†èŠ‚"/></p> <h3 id="prompt-as-prefix">Prompt-as-Prefix</h3> <p>å°†æ—¶é—´åºåˆ—æ•°æ®é›†çš„ä¸€äº›å…ˆéªŒä¿¡æ¯ï¼Œä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼ä½œä¸ºå‰ç¼€ promptï¼Œå¹¶ä¸å¯¹é½åçš„æ—¶åºç‰¹å¾æ‹¼æ¥åè¾“å…¥åˆ° LLMã€‚è¿™å¯ä»¥å‚ç…§æ€»ä½“æ¶æ„å›¾çš„åºå· 4ã€‚ä¸€ä¸ªå¯èƒ½çš„ prompt ç¤ºä¾‹ï¼š <img src="/assets/img/pic/time-llm/prompt.jpg" alt="Prompt ç¤ºä¾‹"/></p> <h3 id="output-projection">Output Projection</h3> <p>ä¸¢å¼ƒå‰ç¼€éƒ¨åˆ†ï¼Œå°†å‰©ä½™éƒ¨åˆ†æ‰å¹³åŒ–å¤„ç†ï¼Œç„¶åé€šè¿‡çº¿æ€§æŠ•å½±æ˜ å°„åˆ°æœ€ç»ˆç»“æœçš„æ ¼å¼ä¸Šã€‚</p> <h2 id="4-æ•°æ®é›†">4. æ•°æ®é›†</h2> <ul> <li>é•¿æœŸï¼šETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI</li> <li>çŸ­æœŸï¼šM4 benchmark</li> </ul> <h2 id="5-å®éªŒç»“æœ">5. å®éªŒç»“æœ</h2> <ul> <li>é•¿æœŸã€çŸ­æœŸ</li> <li>Few-shot on 10%/5%</li> <li>Zero-shot</li> </ul> <p>æ•´ä¸ªæ•ˆæœéƒ½ä¸é”™ï¼Œå…·ä½“æ•°æ®å¯è§åŸè®ºæ–‡çš„Expéƒ¨åˆ†ï¼Œè¿™é‡Œçœç•¥ã€‚</p> <h2 id="6-å¤ç°æƒ…å†µ">6. å¤ç°æƒ…å†µ</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html">Paper Summary &amp;amp; Notes</title><link href="https://fscdc.github.io/blog/2024/PaperNotes/" rel="alternate" type="text/html" title="Paper Summary &amp;amp; Notes"/><published>2024-08-01T00:32:13+00:00</published><updated>2024-08-01T00:32:13+00:00</updated><id>https://fscdc.github.io/blog/2024/PaperNotes</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/PaperNotes/"><![CDATA[<h2 id="index-for-various-fields">Index for Various Fields</h2> <ul> <li> <p>LLM-based time series analysis. <a href="https://fscdc.github.io/blog/2024/model4ts/">LINK</a></p> </li> <li> <p>Backdoor attacks. <a href="https://fscdc.github.io/blog/2024/backdoor/">LINK</a>()</p> </li> <li> <p>AI4Bio.(single cell, etc.) <a href="https://fscdc.github.io/blog/2024/bio/">LINK</a></p> </li> <li> <p>Efficient AI. <a href="https://fscdc.github.io/blog/2024/efficient-llm/">LINK</a></p> </li> <li> <p>Pruning. <a href="https://fscdc.github.io/blog/2024/pruning/">LINK</a></p> </li> <li> <p>Interesting papers. <a href="https://fscdc.github.io/blog/2024/interesting/">LINK</a></p> </li> </ul>]]></content><author><name></name></author><category term="summary"/><category term="note"/><summary type="html"><![CDATA[this is my notes for papers]]></summary></entry><entry><title type="html">Model for Time Series</title><link href="https://fscdc.github.io/blog/2024/model4ts/" rel="alternate" type="text/html" title="Model for Time Series"/><published>2024-05-21T00:13:10+00:00</published><updated>2024-05-21T00:13:10+00:00</updated><id>https://fscdc.github.io/blog/2024/model4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/model4ts/"><![CDATA[<p>This Page collects the papers and codes of Large Language Models (LLMs) and Foundation Models (FMs) for Time Series (TS).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">After</span> <span class="n">the</span> <span class="n">success</span> <span class="n">of</span> <span class="n">BERT</span><span class="p">,</span> <span class="n">GPT</span><span class="p">,</span> <span class="ow">and</span> <span class="n">other</span> <span class="n">LLMs</span> <span class="ow">in</span> <span class="n">NLP</span><span class="p">,</span> <span class="n">some</span> <span class="n">researchers</span> <span class="n">have</span> <span class="n">proposed</span> <span class="n">to</span> <span class="nb">apply</span> <span class="n">LLMs</span> <span class="n">to</span> <span class="n">Time</span> <span class="nc">Series </span><span class="p">(</span><span class="n">TS</span><span class="p">)</span> <span class="n">tasks</span><span class="p">.</span> <span class="n">They</span> <span class="n">fintune</span> <span class="n">the</span> <span class="n">LLMs</span> <span class="n">on</span> <span class="n">TS</span> <span class="n">datasets</span> <span class="ow">and</span> <span class="n">achieve</span> <span class="n">SOTA</span> <span class="n">results</span><span class="p">.</span>
</code></pre></div></div> <h2 id="-llms-for-time-series">ğŸ¦™ LLMs for Time Series</h2> <ul> <li> <p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. <a href="https://arxiv.org/abs/2310.01728">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/time-llm/">[Note]</a></p> </li> <li> <p>TEST: Text Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series. <a href="https://arxiv.org/abs/2308.08241">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/test/">[Note]</a></p> </li> <li> <p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2210.08964">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/promptcast/">[Note]</a></p> </li> <li> <p>One Fits All: Power General Time Series Analysis by Pretrained LM, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2302.11939">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/onefitsall/">[Note]</a></p> </li> <li> <p>Temporal Data Meets LLM â€“ Explainable Financial Time Series Forecasting, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2306.11025">[Paper]</a></p> </li> <li> <p>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. <a href="https://arxiv.org/abs/2308.08469">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/llm4ts/">[Note]</a></p> </li> <li> <p>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. <a href="https://arxiv.org/abs/2309.06236">[Paper]</a></p> </li> <li> <p>Large Language Models Are Zero-Shot Time Series Forecasters. <a href="https://arxiv.org/abs/2310.07820">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/llm4zeroshot/">[Note]</a></p> </li> <li> <p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. <a href="https://arxiv.org/abs/2310.04948">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/tempo/">[Note]</a></p> </li> <li> <p>S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting. <a href="https://arxiv.org/pdf/2403.05798.pdf">[Paper]</a></p> </li> </ul> <h3 id="-survey">ğŸ“ Survey</h3> <ul> <li> <p>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. <a href="https://arxiv.org/abs/2310.10196">[Survey]</a></p> </li> <li> <p>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. <a href="https://arxiv.org/abs/2402.02713">[Survey]</a></p> </li> <li> <p>Foundation Models for Time Series Analysis: A Tutorial and Survey <a href="https://arxiv.org/abs/2403.14735">[Survey]</a></p> </li> </ul> <h2 id="-related-fields">ğŸ”— Related Fields</h2> <p><em>Here, some related fields are listed. These fields are not the main focus of this project, but they are also important for understanding how LLMs are applied to other fields rather than NLP and FMs in specific fields are developed.</em></p> <h3 id="-llm-for-recommendation-systems">ğŸ“ LLM for Recommendation Systems</h3> <ul> <li>Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5), in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2203.13366">[Paper]</a></li> <li>LLM4Rec. <a href="https://github.com/WLiK/LLM4Rec">[GitHub]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Model for Time Series]]></summary></entry><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <h2 id="-backdoor-attacks-on-llm-or-time-series">ğŸ“ Backdoor Attacks on LLM or Time Series</h2> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/BL-survey/">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/survey4llmsp/">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/BA4llm/">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <h2 id="-single-cell-foundation-model">ğŸ¦™ Single-cell Foundation Model</h2> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/scGPT/">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry></feed>