<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-08T14:18:39+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Paper Summary &amp;amp; Notes</title><link href="https://fscdc.github.io/blog/2024/PaperNotes/" rel="alternate" type="text/html" title="Paper Summary &amp;amp; Notes"/><published>2024-08-01T00:32:13+00:00</published><updated>2024-08-01T00:32:13+00:00</updated><id>https://fscdc.github.io/blog/2024/PaperNotes</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/PaperNotes/"><![CDATA[<h2 id="index-for-various-fields">Index for Various Fields</h2> <ul> <li> <p>LLM-based time series analysis. <a href="https://fscdc.github.io/blog/2024/model4ts/">LINK</a></p> </li> <li> <p>Backdoor attacks. <a href="https://fscdc.github.io/blog/2024/backdoor/">LINK</a>()</p> </li> <li> <p>AI4Bio.(single cell, etc.) <a href="https://fscdc.github.io/blog/2024/bio/">LINK</a></p> </li> <li> <p>Efficient AI. <a href="https://fscdc.github.io/blog/2024/efficient-llm/">LINK</a></p> </li> <li> <p>Pruning. <a href="https://fscdc.github.io/blog/2024/pruning/">LINK</a></p> </li> <li> <p>Interesting papers. <a href="https://fscdc.github.io/blog/2024/interesting/">LINK</a></p> </li> </ul>]]></content><author><name></name></author><category term="summary"/><category term="note"/><summary type="html"><![CDATA[this is my notes for papers]]></summary></entry><entry><title type="html">Model for Time Series</title><link href="https://fscdc.github.io/blog/2024/model4ts/" rel="alternate" type="text/html" title="Model for Time Series"/><published>2024-05-21T00:13:10+00:00</published><updated>2024-05-21T00:13:10+00:00</updated><id>https://fscdc.github.io/blog/2024/model4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/model4ts/"><![CDATA[<p>This Page collects the papers and codes of Large Language Models (LLMs) and Foundation Models (FMs) for Time Series (TS).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">After</span> <span class="n">the</span> <span class="n">success</span> <span class="n">of</span> <span class="n">BERT</span><span class="p">,</span> <span class="n">GPT</span><span class="p">,</span> <span class="ow">and</span> <span class="n">other</span> <span class="n">LLMs</span> <span class="ow">in</span> <span class="n">NLP</span><span class="p">,</span> <span class="n">some</span> <span class="n">researchers</span> <span class="n">have</span> <span class="n">proposed</span> <span class="n">to</span> <span class="nb">apply</span> <span class="n">LLMs</span> <span class="n">to</span> <span class="n">Time</span> <span class="nc">Series </span><span class="p">(</span><span class="n">TS</span><span class="p">)</span> <span class="n">tasks</span><span class="p">.</span> <span class="n">They</span> <span class="n">fintune</span> <span class="n">the</span> <span class="n">LLMs</span> <span class="n">on</span> <span class="n">TS</span> <span class="n">datasets</span> <span class="ow">and</span> <span class="n">achieve</span> <span class="n">SOTA</span> <span class="n">results</span><span class="p">.</span>
</code></pre></div></div> <h2 id="-llms-for-time-series">ü¶ô LLMs for Time Series</h2> <ul> <li> <p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. <a href="https://arxiv.org/abs/2310.01728">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/time-llm.md">[Note]</a></p> </li> <li> <p>TEST: Text Prototype Aligned Embedding to Activate LLM‚Äôs Ability for Time Series. <a href="https://arxiv.org/abs/2308.08241">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/test.md">[Note]</a></p> </li> <li> <p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2210.08964">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/promptcast.md">[Note]</a></p> </li> <li> <p>One Fits All: Power General Time Series Analysis by Pretrained LM, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2302.11939">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/onefitsall.md">[Note]</a></p> </li> <li> <p>Temporal Data Meets LLM ‚Äì Explainable Financial Time Series Forecasting, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2306.11025">[Paper]</a></p> </li> <li> <p>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. <a href="https://arxiv.org/abs/2308.08469">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/llm4ts.md">[Note]</a></p> </li> <li> <p>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. <a href="https://arxiv.org/abs/2309.06236">[Paper]</a></p> </li> <li> <p>Large Language Models Are Zero-Shot Time Series Forecasters. <a href="https://arxiv.org/abs/2310.07820">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/llm4zeroshot/">[Note]</a></p> </li> <li> <p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. <a href="https://arxiv.org/abs/2310.04948">[Paper]</a> <a href="https://github.com/fscdc/fscdc.github.io/tree/master/_posts/tempo.md">[Note]</a></p> </li> <li> <p>S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting. <a href="https://arxiv.org/pdf/2403.05798.pdf">[Paper]</a></p> </li> </ul> <h3 id="-survey">üìç Survey</h3> <ul> <li> <p>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. <a href="https://arxiv.org/abs/2310.10196">[Survey]</a></p> </li> <li> <p>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. <a href="https://arxiv.org/abs/2402.02713">[Survey]</a></p> </li> <li> <p>Foundation Models for Time Series Analysis: A Tutorial and Survey <a href="https://arxiv.org/abs/2403.14735">[Survey]</a></p> </li> </ul> <h2 id="-related-fields">üîó Related Fields</h2> <p><em>Here, some related fields are listed. These fields are not the main focus of this project, but they are also important for understanding how LLMs are applied to other fields rather than NLP and FMs in specific fields are developed.</em></p> <h3 id="-llm-for-recommendation-systems">üìç LLM for Recommendation Systems</h3> <ul> <li>Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5), in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2203.13366">[Paper]</a></li> <li>LLM4Rec. <a href="https://github.com/WLiK/LLM4Rec">[GitHub]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Model for Time Series]]></summary></entry><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <h2 id="-backdoor-attacks-on-llm-or-time-series">üìç Backdoor Attacks on LLM or Time Series</h2> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/BL-survey/">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/survey4llmsp/">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/BA4llm/">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <h2 id="-single-cell-foundation-model">ü¶ô Single-cell Foundation Model</h2> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/scGPT/">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry><entry><title type="html">Efficient LLM</title><link href="https://fscdc.github.io/blog/2024/efficient-llm/" rel="alternate" type="text/html" title="Efficient LLM"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/efficient-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/efficient-llm/"><![CDATA[<p>This Page collects the papers and codes of Efficient AI, Efficient Large Language Models (LLMs). Additional, I read paper and take notes.</p> <h2 id="-efficient-llm">ü¶ô Efficient LLM</h2> <h3 id="survey">Survey</h3> <ul> <li>Efficient Large Language Models: A Survey, <a href="https://arxiv.org/abs/2312.03863">Arxiv</a>, <a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Github repo</a></li> </ul> <h2 id="books--courses">Books &amp; Courses</h2> <ul> <li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han (I may update some my learning notes later on my <a href="https://fscdc.github.io/">homepage</a>)</li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Efficient LLM]]></summary></entry><entry><title type="html">Interesting Paper</title><link href="https://fscdc.github.io/blog/2024/interesting/" rel="alternate" type="text/html" title="Interesting Paper"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/interesting</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/interesting/"><![CDATA[<p>This Page collects the papers and codes of which attracted my interests. Additional, I read paper and take notes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">effective</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">multimodal</span><span class="p">,</span> <span class="n">cross</span><span class="o">-</span><span class="n">discipline</span><span class="p">,</span> <span class="n">Leverage</span> <span class="n">Learning</span>
</code></pre></div></div> <h2 id="-interesting-paper">ü¶ô Interesting Paper</h2> <ul> <li> <p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces, in <em>arXiv</em> 2023. <a href="https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf">[Paper]</a></p> </li> <li> <p>Token-Efficient Leverage Learning in Large Language Models, in <em>arXiv</em> 2024. <a href="https://arxiv.org/pdf/2404.00914.pdf">[Paper]</a></p> </li> <li> <p>Heterogeneous Graph Neural Network, in <em>ACM</em> 2019 <a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330961">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/het/">[Note]</a></p> </li> <li> <p>S4 model, <a href="https://arxiv.org/pdf/2111.00396.pdf">[Paper]</a> <a href="https://fscdc.github.io/blog/2024/s4/">[Note]</a></p> </li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Interesting Paper]]></summary></entry><entry><title type="html">Pruning</title><link href="https://fscdc.github.io/blog/2024/pruning/" rel="alternate" type="text/html" title="Pruning"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/pruning</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/pruning/"><![CDATA[<p>TODO</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[pruning]]></summary></entry><entry><title type="html">Heterogeneous Graph Neural Network</title><link href="https://fscdc.github.io/blog/2024/het/" rel="alternate" type="text/html" title="Heterogeneous Graph Neural Network"/><published>2024-05-19T00:13:14+00:00</published><updated>2024-05-19T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/het</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/het/"><![CDATA[<h1 id="heterogeneous-graph-neural-network">Heterogeneous Graph Neural Network</h1> <h2 id="1-introduction">1. Introduction</h2> <p>This paper addresses the representation learning challenges of content-associated heterogeneous graphs (HetG). Traditional works primarily focused on homogeneous structural information and ignored the rich, diverse content across different node types, such as textual, attribute, and image data. We introduce HetGNN, a model designed to encapsulate both the structural and content heterogeneity within graphs.</p> <h2 id="2-methodology">2. Methodology</h2> <h3 id="21-challenges-addressed">2.1 Challenges Addressed</h3> <p>HetGNN tackles several key challenges in the realm of heterogeneous graphs:</p> <ul> <li><strong>C1: Sampling relevant heterogeneous neighbors:</strong> Unlike most GNNs, which only aggregate first-degree neighbors, HetGNN utilizes a random walk with restart strategy to sample a diverse set of strongly associated heterogeneous neighbors.</li> <li><strong>C2: Encoding diverse content information:</strong> Nodes in HetG have varied content types. HetGNN employs a content encoder using RNNs to model the intricate interactions of these content types deeply.</li> <li><strong>C3: Aggregating heterogeneous neighbor features:</strong> To account for the varying influences of different neighbor types, HetGNN integrates a type-based attention mechanism, ensuring that each node type‚Äôs contribution is weighted appropriately.</li> </ul> <h3 id="22-hetgnn-architecture">2.2 HetGNN Architecture</h3> <p>HetGNN‚Äôs architecture comprises two main modules:</p> <ul> <li><strong>Content Encoding Module:</strong> Uses a Bi-LSTM to capture deep feature interactions of node content, resulting in robust content embeddings.</li> <li><strong>Neighbor Aggregation Module:</strong> Aggregates embeddings of different neighbor types using another Bi-LSTM, with attention mechanisms to adjust the influence of each type on the final node embedding.</li> </ul> <h2 id="3-experiments-and-results">3. Experiments and Results</h2> <p>HetGNN was evaluated against state-of-the-art models across multiple datasets and graph mining tasks, including link prediction, recommendation, node classification, and clustering, both in transductive and inductive settings. The results demonstrate that HetGNN consistently outperforms existing methods, particularly in environments rich in node content information.</p> <h2 id="4-contributions-and-model-advantages">4. Contributions and Model Advantages</h2> <p>HetGNN significantly advances the field by:</p> <ul> <li><strong>Defining the heterogeneous graph representation learning problem</strong> that involves both structural and content heterogeneity.</li> <li><strong>Developing a robust model</strong> that effectively captures both elements, applicable to both transductive and inductive tasks.</li> <li><strong>Achieving state-of-the-art performance</strong> on multiple graph mining tasks, demonstrating the effectiveness of our dual-module architecture and the importance of considering both node and edge types in heterogeneous graphs.</li> </ul> <h2 id="5-conclusion">5. Conclusion</h2> <p>HetGNN represents a comprehensive approach to the challenges of heterogeneous graph analysis. The model‚Äôs ability to integrate and learn from both the structural connections and the rich content of nodes leads to superior performance and broad applicability. This work not only sets a new benchmark for heterogeneous graph neural networks but also opens new avenues for future research in this area.</p>]]></content><author><name></name></author><category term="fsc-note"/><category term="note"/><summary type="html"><![CDATA[Heterogeneous Graph Neural Network]]></summary></entry><entry><title type="html">Large Language Models Are Zero-Shot Time Series Forecasters</title><link href="https://fscdc.github.io/blog/2024/llm4zeroshot/" rel="alternate" type="text/html" title="Large Language Models Are Zero-Shot Time Series Forecasters"/><published>2024-05-19T00:13:14+00:00</published><updated>2024-05-19T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/llm4zeroshot</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/llm4zeroshot/"><![CDATA[<h1 id="large-language-models-are-zero-shot-time-series-forecasters">Large Language Models Are Zero-Shot Time Series Forecasters</h1> <h2 id="1-Á†îÁ©∂ËÉåÊôØÂä®Êú∫">1. Á†îÁ©∂ËÉåÊôØ/Âä®Êú∫</h2> <p>Âíå‰πãÂâçÊñáÁ´†Â∑Æ‰∏çÂ§öÔºå‰∏çÂÜçËµòËø∞</p> <h2 id="2-ÂàõÊñ∞ÁÇπ">2. ÂàõÊñ∞ÁÇπ</h2> <p>ÊñπÊ≥ïËôΩÁÑ∂ÊØîËæÉÂπ≥Âá°Ôºå‰ΩÜÂèØËÉΩÂú®ÂΩìÊó∂ÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÊñ∞È¢ñÊÄß„ÄÇ</p> <h2 id="3-‰∏ªË¶ÅÊñπÊ≥ï">3. ‰∏ªË¶ÅÊñπÊ≥ï</h2> <p>Êú¨Á†îÁ©∂ÁöÑÊÄùË∑ØÁõ¥ËßÇÁÆÄÂçïÔºöËæìÂÖ•Áî±Êó∂Â∫èÊï∞ÂÄºÁªÑÊàêÁöÑÂè•Â≠êÔºåÈ¢ÑÊµãÂêéÁª≠Êï∞ÂÄºÁªÑÊàêÁöÑÂè•Â≠ê„ÄÇ</p> <h3 id="tokenÊ†áËÆ∞">TokenÊ†áËÆ∞</h3> <p>Áî±‰∫éÊ®°Âûã‰∏≠Â≠òÂú®ÁöÑÂêÑÁßçÊ†áËÆ∞ÈóÆÈ¢òÔºå‰ΩúËÄÖÈÄâÊã©Âú®ÊØè‰∏™Êï∞Â≠óÈó¥Âä†ÂÖ•Âá†‰∏™ÈÄóÂè∑Ôºå‰ª•Âº∫Âà∂ËßÑÂÆöÊ†áËÆ∞ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊòØÂê¶Âä†Á©∫Ê†ºÂàôÊ†πÊçÆ‰∏çÂêåÁöÑLLMËøõË°åË∞ÉÊï¥„ÄÇ</p> <h3 id="rescaling">Rescaling</h3> <p>‰∏∫‰∫ÜÈÅøÂÖçÊüê‰∫õÊï∞ÂÄºËøáÂ§ßÔºåË¶ÜÁõñ‰∫ÜËøáÂ§öÁöÑtokenÔºåÊï∞ÊçÆÈúÄË¶ÅËøõË°åÈ¢ÑÂ§ÑÁêÜÔºå‰æãÂ¶ÇËøõË°åÁº©Êîæ„ÄÇ‰∏çÂêåÁöÑLLM‰πãÈó¥ÂÖ∑‰ΩìÁöÑÁº©ÊîæÊñπÊ≥ïÊúâÊâÄ‰∏çÂêå„ÄÇ <img src="/assets/img/pic/llm4zeroshot/scaling.jpg" alt="Rescaling ÊñπÊ≥ïÁ§∫ÊÑèÂõæ"/></p> <h3 id="sampling--forecasting">Sampling / Forecasting</h3> <p>ÊØèÊ¨°È¢ÑÊµãÊó∂ÔºåÈÄöËøáÂ§öÊ¨°ÈááÊ†∑ÂÆûÈ™åËé∑ÂæóÂ§öÁªÑÈ¢ÑÊµãÂÄºÔºåÂèñËøô‰∫õÈ¢ÑÊµãÂÄºÁöÑ‰∏≠‰ΩçÊï∞ÊàñÂùáÂÄº‰Ωú‰∏∫ÁÇπÈ¢ÑÊµãÁöÑÁªìÊûúÔºå‰ª•Â¢ûÂä†ÁªìÊûúÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ</p> <h3 id="continuous-likelihoods">Continuous Likelihoods</h3> <p>LLMÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÊòØÁ¶ªÊï£ÁöÑÔºåÈúÄË¶ÅÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ËøûÁª≠Ê¶ÇÁéáÂØÜÂ∫¶ÔºåÊñπÊ≥ïÊòØÁÆÄÂçïÂú∞Âú®ÊÆµÂÜÖËµã‰∫àÂùáÂåÄÂàÜÂ∏É„ÄÇ <img src="/assets/img/pic/llm4zeroshot/p.jpg" alt="ËøûÁª≠Ê¶ÇÁéáÂØÜÂ∫¶Á§∫ÊÑèÂõæ"/></p> <h3 id="language-models-as-flexible-distributions">Language Models as Flexible Distributions</h3> <p>Â∫èÂàóÈ¢ÑÊµãÊú¨Ë¥®‰∏äÊòØÂØπÊú™Êù•ÂÄºÁöÑÊù°‰ª∂ÂàÜÂ∏ÉËøõË°åÂª∫Ê®°ÔºåÂõ†Ê≠§LLMËá™ÁÑ∂‰πüÈÄÇÁî®‰∫éÊ≠§Á±ª‰ªªÂä°„ÄÇ</p> <h2 id="4-Êï∞ÊçÆÈõÜ">4. Êï∞ÊçÆÈõÜ</h2> <p>‰ΩøÁî®‰∫Ü Darts„ÄÅMonash„ÄÅInformer Á≠âÊï∞ÊçÆÈõÜÔºàÂÖ∑‰ΩìËßÅËÆ∫ÊñáÔºâ„ÄÇ</p> <h2 id="5-ÂÆûÈ™åÁªìÊûú">5. ÂÆûÈ™åÁªìÊûú</h2> <p>‰ΩúËÄÖÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÂÆûÈ™åÁªìÊûúÔºå‰ΩÜÂú®Ê≠§ÁúÅÁï•Áõ∏ÂÖ≥ÁªÜËäÇ„ÄÇ</p> <h2 id="6-ÂÆûÈ™åÁéØÂ¢É">6. ÂÆûÈ™åÁéØÂ¢É</h2> <p>ÂÖ∑‰ΩìÂÆûÈ™åÊù°‰ª∂Êú™ËØ¶ÁªÜËØ¥ÊòéÔºå‰∏îÊú™ÂºÄÊ∫ê</p>]]></content><author><name></name></author><category term="fsc-note"/><category term="note"/><summary type="html"><![CDATA[Large Language Models Are Zero-Shot Time Series Forecasters]]></summary></entry><entry><title type="html">S4</title><link href="https://fscdc.github.io/blog/2024/s4/" rel="alternate" type="text/html" title="S4"/><published>2024-05-19T00:13:14+00:00</published><updated>2024-05-19T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/s4</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/s4/"><![CDATA[<p>Ëøô‰∏™<a href="https://www.youtube.com/watch?v=c1uNEvU0Mbo">Youtuber</a>ËÆ≤ÁöÑÂèØ‰ª•ÔºåÂêåÊó∂ÈúÄË¶ÅËá™Â∑±Êü•‰∏Ä‰∫õbasic knowledgeÔºåÊàëÂ∞±‰∏çËµòËø∞‰∫Ü„ÄÇ</p>]]></content><author><name></name></author><category term="fsc-note"/><category term="note"/><summary type="html"><![CDATA[Ëøô‰∏™YoutuberËÆ≤ÁöÑÂèØ‰ª•ÔºåÂêåÊó∂ÈúÄË¶ÅËá™Â∑±Êü•‰∏Ä‰∫õbasic knowledgeÔºåÊàëÂ∞±‰∏çËµòËø∞‰∫Ü„ÄÇ]]></summary></entry></feed>